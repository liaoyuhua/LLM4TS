<div align='center'>

[English](README.md) | ç®€ä½“ä¸­æ–‡

# LLM4TS: Large Language Models for Time Series

</div>

è¿™ä¸ªé¡¹ç›®æ”¶é›†äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨æ—¶é—´åºåˆ—ï¼ˆTSï¼‰æ–¹é¢çš„è®ºæ–‡å’Œä»£ç ã€‚å¸Œæœ›è¿™ä¸ªé¡¹ç›®èƒ½å¸®åŠ©ä½ ç†è§£LLMså’ŒFMsåœ¨æ—¶é—´åºåˆ—æ–¹é¢çš„åº”ç”¨ã€‚

## ğŸ¦™ å¤§è¯­è¨€æ¨¡å‹ä¸æ—¶é—´åºåˆ—

*åœ¨BERTã€GPTå’Œå…¶ä»–LLMsåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—æˆåŠŸåï¼Œä¸€äº›ç ”ç©¶è€…æå‡ºå°†LLMsåº”ç”¨äºæ—¶é—´åºåˆ—ï¼ˆTSï¼‰ä»»åŠ¡ã€‚ä»–ä»¬åœ¨TSæ•°æ®é›†ä¸Šå¯¹LLMsè¿›è¡Œå¾®è°ƒï¼Œå¹¶å–å¾—äº†SOTAç»“æœã€‚*

* PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting Hao, in *arXiv* 2022. [\[Paper\]](https://arxiv.org/abs/2210.08964)
* One Fits All: Power General Time Series Analysis by Pretrained LM, in *arXiv* 2023. [\[Paper\]](https://arxiv.org/abs/2302.11939)
* Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting, in *arXiv* 2023. [\[Paper\]](https://arxiv.org/abs/2306.11025)
* TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. [\[Paper\]](https://arxiv.org/abs/2308.08241)
* LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. [\[Paper\]](https://arxiv.org/abs/2308.08469)

* The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. [\[Paper\]](https://arxiv.org/abs/2309.06236)

* Large Language Models Are Zero-Shot Time Series Forecasters. [\[Paper\]](https://arxiv.org/abs/2310.07820)

* TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. [\[Paper\]](https://arxiv.org/abs/2310.04948)

* Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. [\[Paper\]](https://arxiv.org/abs/2310.01728)

### ğŸ“ ç»¼è¿°

* Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. [\[Survey\]](https://arxiv.org/abs/2310.10196)

* Position Paper: What Can Large Language Models Tell Us about Time Series Analysis. [\[Survey\]](https://arxiv.org/abs/2402.02713)

* Foundation Models for Time Series Analysis: A Tutorial and Survey [\[Survey\]](https://arxiv.org/abs/2403.14735)

### ğŸ“ ç±»ä¼¼çš„å·¥ä½œ
* Large Language Models are Few-Shot Health Learners, in *arXiv* 2023. [\[Paper\]](https://arxiv.org/abs/2305.15525)

* Frozen Language Model Helps ECG Zero-Shot Learning, in *arXiv* 2023.[\[Paper\]](https://arxiv.org/abs/2303.12311)

## ğŸ§± åŸºåº§æ¨¡å‹ä¸æ—¶é—´åºåˆ—

*æœ€è¿‘ï¼Œä¸€äº›æ—¶é—´åºåˆ—ï¼ˆTSï¼‰çš„åŸºåº§æ¨¡å‹ï¼ˆFMsï¼‰è¢«æå‡ºã€‚è¿™äº›FMsæ—¨åœ¨ä»å¤§å‹æ•°æ®é›†ä¸­å­¦ä¹ æ—¶é—´åºåˆ—çš„è¡¨ç¤ºï¼Œå¹¶å°†è¡¨ç¤ºè½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­ã€‚ä¸TS-LLMç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•ä¸ä¾èµ–é¢„è®­ç»ƒçš„LLMsã€‚*

* Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. [\[Paper\]](https://arxiv.org/abs/2401.03955)

* A decoder-only foundation model for time-series forecasting. [\[Paper\]](https://arxiv.org/abs/2310.10688)

* TimeGPT-1. [\[Paper\]](https://arxiv.org/abs/2310.03589?ref=emergentmind)

* Lag-Llama: Towards Foundation Models for Time Series Forecasting. [\[Paper\]](https://arxiv.org/abs/2310.08278)

* Unified Training of Universal Time Series Forecasting Transformers. [\[Paper\]](https://arxiv.org/abs/2402.02592)

* MOMENT: A Family of Open Time-series Foundation Models. [\[Paper\]](https://arxiv.org/abs/2402.03885)

## ğŸ”— ç›¸å…³é¢†åŸŸ
*è¿™é‡Œåˆ—å‡ºäº†ä¸€äº›ç›¸å…³é¢†åŸŸã€‚è¿™äº›é¢†åŸŸå¹¶éæœ¬é¡¹ç›®çš„ä¸»è¦å…³æ³¨ç‚¹ï¼Œä½†å®ƒä»¬å¯¹äºç†è§£LLMså¦‚ä½•åº”ç”¨äºé™¤äº†NLPä¹‹å¤–çš„å…¶ä»–é¢†åŸŸï¼Œä»¥åŠå…¶ä»–ç‰¹å®šé¢†åŸŸå¦‚ä½•æ„å»ºåŸºåº§æ¨¡å‹ä¹Ÿå¾ˆé‡è¦ã€‚*

### ğŸ“ é¢„è®­ç»ƒæ—¶é—´åºåˆ—æ¨¡å‹
* A Survey on Time-Series Pre-Trained Models, in *arXiv* 2023. [\[Paper\]](https://arxiv.org/abs/2305.10716)
* Transfer learning for Time Series Forecasting. [\[GitHub\]](https://github.com/Nixtla/transfer-learning-time-series)
* TST: A transformer-based framework for multi- variate time series representation learning. [\[Paper\]](https://arxiv.org/abs/2010.02803)
* Ti-mae: Self-supervised masked time series autoencoders. [\[Paper\]](https://arxiv.org/abs/2301.08871)
* SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling. [\[Paper\]](https://arxiv.org/pdf/2302.00861.pdf)

* Cost: Contrastive learning of disentangled seasonal-trend rep- resentations for time series forecasting.[\[Paper\]](https://arxiv.org/abs/2202.01575)

* TS2Vec: Towards Universal Representation of Time Series. [\[Paper\]](https://arxiv.org/abs/2106.10466)

### ğŸ“ å¤§è¯­è¨€æ¨¡å‹ä¸æ¨èç³»ç»Ÿ
* Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5), in *arXiv* 2022. [\[Paper\]](https://arxiv.org/abs/2203.13366)
* LLM4Rec. [\[GitHub\]](https://github.com/WLiK/LLM4Rec)


### ğŸ“ åŸºåº§æ¨¡å‹ä¸è¡¨æ ¼æ•°æ®
* AnyPredict: Foundation Model for Tabular Prediction, in *arXiv* 2023. [\[Paper\]](https://arxiv.org/abs/2305.12081)
* XTab: Cross-table Pretraining for Tabular Transformers, in *ICML* 2023. [\[Paper\]](https://arxiv.org/abs/2305.06090)

### ğŸ“ LLMOps
* Awesome-LLMOps. [\[GitHub\]](https://github.com/tensorchord/Awesome-LLMOps)
